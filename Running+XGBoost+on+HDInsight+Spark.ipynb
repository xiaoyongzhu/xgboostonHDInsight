{"nbformat_minor": 2, "cells": [{"source": "# Running XGBoost on Azure HDInsight\nThis Notebook will walk you through on the detailed steps on how to build, install, and run XGBoost on HDInsight, the managed Hadoop and Spark solution on Azure.\n![XGBoost](https://raw.githubusercontent.com/dmlc/dmlc.github.io/master/img/logo-m/xgboost.png) ", "cell_type": "markdown", "metadata": {}}, {"source": "### XGBoost\nXGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.\n\nIt is not designed as a generic Machine Learning framework; it is designed as a library very specialized in boosting tree algorithm, and is widely used from production to experimental projects.\n\nFor more details on XGBoost, please go to XGBoost [GitHub page](https://github.com/dmlc/xgboost).\n\n", "cell_type": "markdown", "metadata": {}}, {"source": "### How to use this notebook\nThis notebook basically provides an E2E workflow from building XGBoost jars, deploying the jars to Azure Storage, to running Boosting Tree algorithm to HDInsight.", "cell_type": "markdown", "metadata": {}}, {"source": "### Building XGBoost from source code\nThe following code snippet \n\n- installs the required libraries for building XGBoost\n- builds XGBoost using Maven\n- put the compiled jars to the default storage account of the HDInsight cluster\n- put the sample data to the default storage account of the HDInsight cluster\n\nThe cell below is using the %%sh magic which will execute the code below as bash scripts in the head node.\n\nYou might see something like this when building xgboost. This is expected and the final test should pass.\n\n    Tracker started, with env={DMLC_NUM_SERVER=0, DMLC_TRACKER_URI=10.0.0.15, DMLC_TRACKER_PORT=9091, DMLC_NUM_WORKER=4}\n    17/08/14 22:41:34 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)\n    java.lang.RuntimeException: Worker exception.\n            at ml.dmlc.xgboost4j.scala.spark.RabitTrackerRobustnessSuite$$anonfun$1$$anonfun$2.apply(RabitTrackerRobustnessSuite.scala:72)\n            at ml.dmlc.xgboost4j.scala.spark.RabitTrackerRobustnessSuite$$anonfun$1$$anonfun$2.apply(RabitTrackerRobustnessSuite.scala:66)\n            at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "%%sh\nsudo apt-get update\nsudo apt-get install -y maven git build-essential cmake python-setuptools\ngit clone --recursive https://github.com/dmlc/xgboost\n\n#builds XGBoost using Maven\ncd xgboost/jvm-packages\nmvn -DskipTests=true install\n\n#put the compiled packge to shared storage\n#put to root folder for simplicity\nhadoop fs -put -f xgboost4j-spark/target/xgboost4j-spark-0.7.jar /\nhadoop fs -put -f xgboost4j/target/xgboost4j-0.7.jar /\nhadoop fs -put -f xgboost4j-example/target/xgboost4j-example-0.7.jar /\n\n\n#put the sample data to shared storage\nhadoop fs -put -f ..//demo/data/agaricus.txt* /", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Process is terminated.\n"}], "metadata": {"collapsed": false}}, {"source": "### Start a Spark session\nAfter putting the jars and the files to the Azure Storage, which is shared across all the HDInsight nodes, the next step is to start a Spark session and call the XGBoost libraries. \n\nIn the configure cell below, first we need to load those jar files to the Spark session, so we can use XGBoost APIs in this Jupyter Notebook.\n\nWe also need to exclude a few spark jars because there are some conflicts between Livy (which is the REST API used on HDInsight to execute Spark code), and XGBoost.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%configure -f\n{ \"jars\": [\"wasb:///xgboost4j-spark-0.7.jar\", \"wasb:///xgboost4j-0.7.jar\", \"wasb:///xgboost4j-example-0.7.jar\"],\n  \"conf\": {\n    \"spark.jars.excludes\": \"org.scala-lang:scala-reflect:2.11.8,org.scala-lang:scala-compiler:2.11.8,org.scala-lang:scala-library:2.11.8\"\n   }\n}", "outputs": [{"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "Current session configs: <tt>{u'jars': [u'wasb:///xgboost4j-spark-0.7.jar', u'wasb:///xgboost4j-0.7.jar', u'wasb:///xgboost4j-example-0.7.jar'], u'kind': 'spark', u'conf': {u'spark.jars.excludes': u'org.scala-lang:scala-reflect:2.11.8,org.scala-lang:scala-compiler:2.11.8,org.scala-lang:scala-library:2.11.8'}}</tt><br>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "No active sessions."}, "metadata": {}}], "metadata": {"collapsed": false}}, {"source": "### Import Packages\nWe then import the XGBoost packages and start a Spark application", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "import ml.dmlc.xgboost4j.scala.Booster\nimport ml.dmlc.xgboost4j.scala.spark.XGBoost\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.SparkConf", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Starting Spark application\n"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1502756750987_0005</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hn1-xiaoyz.znlyh0it2tzuza0r3wrc11atlh.ix.internal.cloudapp.net:8088/proxy/application_1502756750987_0005/\">Link</a></td><td><a target=\"_blank\" href=\"http://10.0.0.6:30060/node/containerlogs/container_1502756750987_0005_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "SparkSession available as 'spark'.\nimport org.apache.spark.SparkConf"}], "metadata": {"collapsed": false}}, {"source": "### Train a simple XGBoost model\nWe read data from the default BLOB storage account. The data is already put there if you run the %%sh cell above. But you can also get the data from [XGBoost repo](https://github.com/dmlc/xgboost/tree/master/demo/data).", "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": "// create training and testing dataframes\nval inputTrainPath = \"wasb:///agaricus.txt.train\"\nval inputTestPath = \"wasb:///agaricus.txt.test\"\nval outputModelPath = \"wasb:///XGBoostModelOutput\"\nval numWorkers = 4\n\n// number of iterations\nval numRound = 100\n\n// build dataset\nval trainDF = spark.sqlContext.read.format(\"libsvm\").load(inputTrainPath)\nval testDF = spark.sqlContext.read.format(\"libsvm\").load(inputTestPath)\n// start training\nval paramMap = List(\n  \"eta\" -> 0.1f,\n  \"max_depth\" -> 6,\n  \"objective\" -> \"binary:logistic\").toMap\n\nval xgboostModel = XGBoost.trainWithDataFrame(\n  trainDF, paramMap, numRound, nWorkers = numWorkers, useExternalMemory = true)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "xgboostModel: ml.dmlc.xgboost4j.scala.spark.XGBoostModel = XGBoostClassificationModel_07caca627526"}], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "### Train a simple XGBoost model\nTransform the test data and look at the results", "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": "// xgboost-spark appends the column containing prediction results\nxgboostModel.transform(testDF).show()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----+--------------------+--------------------+----------+\n|label|            features|       probabilities|prediction|\n+-----+--------------------+--------------------+----------+\n|  0.0|(126,[0,8,18,20,2...|[0.99930757284164...|       0.0|\n|  1.0|(126,[2,8,18,20,2...|[0.00261396169662...|       1.0|\n|  0.0|(126,[0,8,19,20,2...|[0.99930757284164...|       0.0|\n|  0.0|(126,[2,8,18,20,2...|[0.99930757284164...|       0.0|\n|  0.0|(126,[3,6,10,21,2...|[0.99719786643981...|       0.0|\n|  0.0|(126,[2,9,19,20,2...|[0.99351829290390...|       0.0|\n|  1.0|(126,[2,8,10,20,2...|[0.00236105918884...|       1.0|\n|  0.0|(126,[0,8,19,20,2...|[0.99931138753890...|       0.0|\n|  1.0|(126,[2,8,18,20,2...|[0.00265699625015...|       1.0|\n|  0.0|(126,[3,8,19,20,2...|[0.99906325340271...|       0.0|\n|  1.0|(126,[2,8,10,20,2...|[0.00236105918884...|       1.0|\n|  0.0|(126,[0,8,19,20,2...|[0.99930530786514...|       0.0|\n|  0.0|(126,[3,6,13,21,2...|[0.99980139732360...|       0.0|\n|  0.0|(126,[0,8,18,20,2...|[0.99930757284164...|       0.0|\n|  0.0|(126,[3,9,10,21,2...|[0.99979490041732...|       0.0|\n|  0.0|(126,[3,8,19,20,2...|[0.99907654523849...|       0.0|\n|  0.0|(126,[0,9,19,20,2...|[0.99925458431243...|       0.0|\n|  1.0|(126,[2,8,10,20,2...|[0.00236105918884...|       1.0|\n|  0.0|(126,[5,6,10,21,2...|[0.99748295545578...|       0.0|\n|  0.0|(126,[2,9,19,20,2...|[0.99925458431243...|       0.0|\n+-----+--------------------+--------------------+----------+\nonly showing top 20 rows"}], "metadata": {"collapsed": false}}, {"source": "### Explain Parameters\nLet's also take a look at the parameters", "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": "xgboostModel.explainParams()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "res47: String =\nbooster: Booster to use, options: {'gbtree', 'gblinear', 'dart'} (default: gbtree, current: gbtree)\nalpha: L1 regularization term on weights, increase this value will make model more conservative. (default: 0.0, current: 0.0)\nbooster: Booster to use, options: {'gbtree', 'gblinear', 'dart'} (default: gbtree, current: gbtree)\ncolsample_bylevel: subsample ratio of columns for each split, in each level. (default: 1.0, current: 1.0)\ncolsample_bytree: subsample ratio of columns when constructing each tree. (default: 1.0, current: 1.0)\neta: step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features. and eta actually shrinks the feature weights to make the boosting process more conservative. (default: 0.3..."}], "metadata": {"collapsed": false}}, {"source": "### Save the model to Azure Storage\nXGBoost can save the model to Azure Storage. We need to specify the implicit value sc, which is required by the saveModelAsHadoopFile API. It is the sparkContext type so we need to get it from the default spark (which is of sparkSession type)", "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": "//set sc value which is required by the saveModelAsHadoopFile API. It is the sparkContext type so we need to get it from the default spark (which is of sparkSession type)\nimplicit val sc = spark.sparkContext\nxgboostModel.saveModelAsHadoopFile(outputModelPath)", "outputs": [], "metadata": {"collapsed": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Spark", "name": "sparkkernel", "language": ""}, "language_info": {"mimetype": "text/x-scala", "pygments_lexer": "scala", "name": "scala", "codemirror_mode": "text/x-scala"}}}